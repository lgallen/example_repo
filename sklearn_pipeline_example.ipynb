{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn Pipeline Example: NCAA Basketball Quadrant Classification\n",
    "\n",
    "This notebook demonstrates a complete machine learning pipeline using scikit-learn with hyperparameter optimization.\n",
    "\n",
    "**Goal**: Predict which \"quadrant\" an NCAA basketball team belongs to based on their performance metrics.\n",
    "\n",
    "**Data Source**: NCAA Men's Basketball NET Rankings (web scraped using `pandas.read_html()`)\n",
    "\n",
    "**Quadrant System (Balanced):**\n",
    "- Teams are divided into 4 equal groups based on their ranking\n",
    "- Each quadrant contains approximately the same number of teams\n",
    "- Quad 1: Top 25% of teams\n",
    "- Quad 2: Second 25% of teams\n",
    "- Quad 3: Third 25% of teams\n",
    "- Quad 4: Bottom 25% of teams\n",
    "\n",
    "**Features Used:**\n",
    "- Wins (numeric)\n",
    "- Losses (numeric)\n",
    "- Conference (categorical)\n",
    "- Non-Division I Wins (numeric)\n",
    "\n",
    "## Key Concepts Covered:\n",
    "1. Data acquisition from the web using `pandas.read_html()`\n",
    "2. Feature engineering (parsing records into wins/losses)\n",
    "3. Multi-class classification (4 balanced classes)\n",
    "4. Preprocessing pipelines (numeric and categorical)\n",
    "5. Column transformers\n",
    "6. **Hyperparameter tuning with GridSearchCV**\n",
    "7. **Cross-validation (5-fold CV)**\n",
    "8. Model evaluation and feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Acquisition - Web Scraping NCAA NET Rankings\n",
    "\n",
    "`pandas.read_html()` is a powerful function that automatically finds and parses HTML tables from web pages.\n",
    "\n",
    "**How it works:**\n",
    "- Takes a URL or HTML string\n",
    "- Returns a list of DataFrames (one for each table found)\n",
    "- Requires `lxml` or `html5lib` parser\n",
    "\n",
    "We'll scrape the NCAA NET Rankings and extract team statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape NCAA NET Rankings data\n",
    "url = 'https://www.ncaa.com/rankings/basketball-men/d1/ncaa-mens-basketball-net-rankings'\n",
    "\n",
    "# Add headers to avoid potential blocking\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Fetch the page with proper headers\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Parse tables from the HTML content\n",
    "    tables = pd.read_html(StringIO(response.text))\n",
    "    \n",
    "    print(f\"✓ Successfully scraped {len(tables)} table(s) from NCAA.com\")\n",
    "    \n",
    "    # Get the main rankings table\n",
    "    df = tables[0].copy()\n",
    "    \n",
    "    # Standardize column names\n",
    "    if 'School' in df.columns and 'Team' not in df.columns:\n",
    "        df['Team'] = df['School']\n",
    "    \n",
    "    print(f\"\\nData shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Web scraping failed: {type(e).__name__}: {e}\")\n",
    "    print(\"\\nNote: If scraping fails, you may need to check the URL or create synthetic data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Parse record into wins and losses\n",
    "# Record is typically in format \"W-L\" like \"25-6\"\n",
    "\n",
    "def parse_record(record_str):\n",
    "    \"\"\"Parse record string into wins and losses\"\"\"\n",
    "    try:\n",
    "        if '-' in str(record_str):\n",
    "            parts = str(record_str).split('-')\n",
    "            if len(parts) >= 2:\n",
    "                wins = int(parts[0])\n",
    "                losses = int(parts[1])\n",
    "                return wins, losses\n",
    "        return None, None\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "# Find the record column (might be named 'Record', 'W-L', or similar)\n",
    "record_col = None\n",
    "for col in df.columns:\n",
    "    if 'record' in col.lower() or 'w-l' in col.lower() or '-' in str(df[col].iloc[0] if len(df) > 0 else ''):\n",
    "        # Check if this looks like a record column\n",
    "        sample = str(df[col].iloc[0]) if len(df) > 0 else ''\n",
    "        if '-' in sample and any(c.isdigit() for c in sample):\n",
    "            record_col = col\n",
    "            break\n",
    "\n",
    "if record_col:\n",
    "    print(f\"Found record column: '{record_col}'\")\n",
    "    df[['Wins', 'Losses']] = df[record_col].apply(lambda x: pd.Series(parse_record(x)))\n",
    "else:\n",
    "    print(\"⚠ Could not find record column. Creating sample data...\")\n",
    "    # Create sample wins/losses based on ranking\n",
    "    df['Wins'] = 30 - (df['Rank'] // 12)  # Better teams have more wins\n",
    "    df['Losses'] = 2 + (df['Rank'] // 30)  # Worse teams have more losses\n",
    "\n",
    "print(\"\\nSample of Wins/Losses:\")\n",
    "print(df[['Rank', 'Team', 'Wins', 'Losses']].head(20))\n",
    "\n",
    "# Check if Conference column exists\n",
    "if 'Conference' not in df.columns:\n",
    "    # Look for similar column names\n",
    "    conf_col = None\n",
    "    for col in df.columns:\n",
    "        if 'conf' in col.lower() or 'league' in col.lower():\n",
    "            conf_col = col\n",
    "            df['Conference'] = df[conf_col]\n",
    "            print(f\"\\n✓ Found conference column: '{conf_col}'\")\n",
    "            break\n",
    "    \n",
    "    if conf_col is None:\n",
    "        print(\"\\n⚠ Conference column not found. Creating synthetic conference data...\")\n",
    "        # Create synthetic conferences based on team distribution\n",
    "        conferences = ['ACC', 'Big Ten', 'Big 12', 'SEC', 'Pac-12', 'Big East', \n",
    "                      'American', 'Mountain West', 'West Coast', 'Atlantic 10', 'Other']\n",
    "        df['Conference'] = np.random.choice(conferences, size=len(df), \n",
    "                                           p=[0.15, 0.15, 0.12, 0.15, 0.10, 0.10, \n",
    "                                             0.08, 0.05, 0.04, 0.04, 0.02])\n",
    "\n",
    "print(\"\\nConference distribution:\")\n",
    "print(df['Conference'].value_counts().head(10))\n",
    "\n",
    "# Create balanced quadrant labels\n",
    "total_teams = len(df)\n",
    "teams_per_quad = total_teams // 4\n",
    "\n",
    "def assign_balanced_quadrant(rank):\n",
    "    \"\"\"Assign quadrant based on equal distribution\"\"\"\n",
    "    if rank <= teams_per_quad:\n",
    "        return 1\n",
    "    elif rank <= teams_per_quad * 2:\n",
    "        return 2\n",
    "    elif rank <= teams_per_quad * 3:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "df['Quadrant'] = df['Rank'].apply(assign_balanced_quadrant)\n",
    "\n",
    "print(\"\\n\\nBalanced Quadrant distribution:\")\n",
    "print(df['Quadrant'].value_counts().sort_index())\n",
    "print(f\"Each quadrant has approximately {teams_per_quad} teams\")\n",
    "\n",
    "print(f\"\\nDataset now has {df.shape[0]} teams with balanced quadrant labels\")\n",
    "df[['Rank', 'Team', 'Wins', 'Losses', 'Conference', 'Quadrant']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Exploration and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the scraped data\n",
    "print(\"Initial Dataset Information:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nAll available columns:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(\"\\n\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n\\nFirst few rows of raw data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract Non-Division I wins\n# This is typically in a column that shows wins against non-D1 opponents\n\nprint(\"Looking for Non-Division I wins column...\")\n\n# Common column names that might contain this info\nnon_d1_col = None\nfor col in df.columns:\n    col_lower = col.lower()\n    if ('non' in col_lower and 'div' in col_lower) or 'non-d1' in col_lower or 'nond1' in col_lower:\n        non_d1_col = col\n        break\n\nif non_d1_col:\n    # The Non-Div I column contains record strings like \"3-0\" (wins-losses format)\n    # We need to parse out just the wins using the parse_record function\n    df['Non_D1_Wins'] = df[non_d1_col].apply(lambda x: parse_record(x)[0] if parse_record(x)[0] is not None else 0)\n    print(f\"✓ Using column: {non_d1_col}\")\nelse:\n    print(\"⚠ Non-D1 wins column not found in data.\")\n    print(\"  Creating synthetic Non-D1 wins (0-3 per team, more for lower-ranked teams)\")\n    # Lower-ranked teams tend to schedule more non-D1 opponents\n    df['Non_D1_Wins'] = np.random.poisson(lam=1.5 - (df['Rank'] / len(df) * 1.5))\n    df['Non_D1_Wins'] = df['Non_D1_Wins'].clip(0, 3)  # Cap at 3\n\nprint(\"\\nNon-D1 Wins statistics:\")\nprint(df['Non_D1_Wins'].describe())\n\nprint(\"\\nSample of all engineered features:\")\nprint(df[['Rank', 'Team', 'Wins', 'Losses', 'Conference', 'Non_D1_Wins']].head(15))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Classification Problem\n",
    "\n",
    "**Task**: Predict which quadrant a team belongs to based on their performance metrics.\n",
    "\n",
    "**Target Variable**: Quadrant (1, 2, 3, or 4) - **Balanced** with equal number of teams in each\n",
    "\n",
    "**Features** (carefully selected):\n",
    "- **Wins** (numeric) - Total wins in the season\n",
    "- **Losses** (numeric) - Total losses in the season\n",
    "- **Conference** (categorical) - Conference affiliation\n",
    "- **Non_D1_Wins** (numeric) - Wins against non-Division I opponents\n",
    "\n",
    "This is a **multi-class classification problem** with 4 balanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the specified features\n",
    "# Features: Wins, Losses, Conference, Non_D1_Wins\n",
    "\n",
    "# Ensure all required columns exist\n",
    "required_features = ['Wins', 'Losses', 'Conference', 'Non_D1_Wins']\n",
    "\n",
    "# Check if all features are available\n",
    "available_features = [col for col in required_features if col in df.columns]\n",
    "missing_features = [col for col in required_features if col not in df.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"⚠ Warning: Missing features: {missing_features}\")\n",
    "    print(\"  These will be excluded from the model.\")\n",
    "\n",
    "# Create feature matrix with only specified features\n",
    "X = df[available_features].copy()\n",
    "y = df['Quadrant'].copy()\n",
    "\n",
    "print(f\"Features (X):\")\n",
    "print(f\"  Shape: {X.shape}\")\n",
    "print(f\"  Columns: {X.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\nTarget (y):\")\n",
    "print(f\"  Shape: {y.shape}\")\n",
    "print(f\"  Class distribution (balanced):\")\n",
    "print(y.value_counts().sort_index())\n",
    "\n",
    "print(\"\\nSample of feature data:\")\n",
    "print(X.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical features\n",
    "print(\"Feature types:\")\n",
    "\n",
    "numeric_features = []\n",
    "categorical_features = []\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].dtype in ['int64', 'float64']:\n",
    "        numeric_features.append(col)\n",
    "        print(f\"  {col}: numeric (dtype: {X[col].dtype})\")\n",
    "    else:\n",
    "        categorical_features.append(col)\n",
    "        print(f\"  {col}: categorical (dtype: {X[col].dtype})\")\n",
    "\n",
    "print(f\"\\nNumeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\\nMissing values:\")\n",
    "missing_counts = X.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "else:\n",
    "    print(\"  None - all features are complete ✓\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\\nSummary statistics for numeric features:\")\n",
    "if numeric_features:\n",
    "    print(X[numeric_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split Data into Training and Testing Sets\n",
    "\n",
    "We'll split our data 80/20 for training and testing, using stratification to ensure each quadrant is proportionally represented in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} teams\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} teams\")\n",
    "\n",
    "print(f\"\\nTraining set quadrant distribution:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nTesting set quadrant distribution:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that stratification worked correctly\n",
    "train_proportions = y_train.value_counts(normalize=True).sort_index()\n",
    "test_proportions = y_test.value_counts(normalize=True).sort_index()\n",
    "\n",
    "print(\"Quadrant proportions (should be similar in train and test):\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Training': train_proportions,\n",
    "    'Testing': test_proportions\n",
    "})\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build the Pipeline\n",
    "\n",
    "This is the key part! We'll create a complete pipeline that:\n",
    "1. Handles numeric features (imputation + scaling)\n",
    "2. Handles categorical features (imputation + one-hot encoding)\n",
    "3. Trains a multi-class classifier\n",
    "\n",
    "All in one convenient package!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build the Pipeline\n",
    "\n",
    "This is the key part! We'll create a complete pipeline that:\n",
    "1. Handles numeric features (imputation + scaling)\n",
    "2. Handles categorical features (imputation + one-hot encoding)\n",
    "3. Trains a classifier\n",
    "\n",
    "All in one convenient package!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build preprocessing transformers for our specific features\n",
    "# Numeric: Wins, Losses, Non_D1_Wins\n",
    "# Categorical: Conference\n",
    "\n",
    "print(\"Building preprocessing pipeline...\")\n",
    "\n",
    "# Numeric transformer: impute missing values (if any), then scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical transformer: impute missing values, then one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "print(\"✓ Preprocessing pipeline created!\")\n",
    "print(f\"\\nNumeric features to be scaled: {numeric_features}\")\n",
    "print(f\"Categorical features to be one-hot encoded: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipelines for both numeric and categorical data\n",
    "\n",
    "# Numeric transformer: impute missing values, then scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical transformer: impute missing values, then one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "print(\"Preprocessing pipeline created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the complete pipeline: preprocessing + classifier\n",
    "# We'll use GridSearchCV to find the best hyperparameters\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "print(\"Pipeline structure:\")\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "Instead of just training with default parameters, we'll use GridSearchCV to find the best hyperparameters for our Random Forest classifier through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define the parameter grid for Random Forest\nparam_grid = {\n    'classifier__n_estimators': [50, 100, 200],\n    'classifier__max_depth': [None, 10, 20, 30],\n    'classifier__min_samples_split': [2, 5, 10],\n    'classifier__min_samples_leaf': [1, 2, 4]\n}\n\nprint(\"Hyperparameter grid to search:\")\nfor param, values in param_grid.items():\n    print(f\"  {param}: {values}\")\n\nprint(f\"\\nTotal combinations to test: {np.prod([len(v) for v in param_grid.values()])}\")\n\n# Create GridSearchCV\ngrid_search = GridSearchCV(\n    pipeline,\n    param_grid=param_grid,\n    cv=5,  # 5-fold cross-validation\n    scoring='accuracy',\n    n_jobs=-1,  # Use all available processors\n    verbose=1,\n    return_train_score=True\n)\n\nprint(\"\\nStarting Grid Search with 5-fold Cross-Validation...\")\nprint(\"This may take a few minutes...\\n\")\n\n# Fit the grid search\ngrid_search.fit(X_train, y_train)\n\nprint(\"\\n✓ Grid Search Complete!\")\nprint(f\"\\nBest parameters found:\")\nfor param, value in grid_search.best_params_.items():\n    print(f\"  {param}: {value}\")\n\nprint(f\"\\nBest cross-validation accuracy: {grid_search.best_score_:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Grid Search Results\n",
    "# Show top performing parameter combinations\n",
    "\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Select relevant columns\n",
    "results_summary = results_df[[\n",
    "    'param_classifier__n_estimators',\n",
    "    'param_classifier__max_depth',\n",
    "    'param_classifier__min_samples_split',\n",
    "    'param_classifier__min_samples_leaf',\n",
    "    'mean_test_score',\n",
    "    'std_test_score',\n",
    "    'rank_test_score'\n",
    "]].sort_values('rank_test_score').head(10)\n",
    "\n",
    "print(\"Top 10 Parameter Combinations:\")\n",
    "print(results_summary.to_string(index=False))\n",
    "\n",
    "print(f\"\\n\\nComparison:\")\n",
    "print(f\"  Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"  Worst CV Score: {results_df['mean_test_score'].min():.4f}\")\n",
    "print(f\"  Improvement: {(grid_search.best_score_ - results_df['mean_test_score'].min()):.4f}\")\n",
    "\n",
    "# Visualize parameter importance\n",
    "print(\"\\n\\nParameter Value Impact on Performance:\")\n",
    "for param in param_grid.keys():\n",
    "    param_col = f'param_{param}'\n",
    "    avg_by_param = results_df.groupby(param_col)['mean_test_score'].mean().sort_values(ascending=False)\n",
    "    print(f\"\\n{param}:\")\n",
    "    for val, score in avg_by_param.items():\n",
    "        print(f\"  {val}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate the optimized model on the held-out test set\ny_pred = grid_search.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Set Accuracy: {accuracy:.3f}\")\nprint(f\"(Compared to best CV accuracy: {grid_search.best_score_:.3f})\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Quad 1', 'Quad 2', 'Quad 3', 'Quad 4']))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Confusion matrix\ncm = confusion_matrix(y_test, grid_search.predict(X_test))\nprint(\"Confusion Matrix:\")\nprint(\"Rows = Actual, Columns = Predicted\")\nprint(\"\\n      Q1   Q2   Q3   Q4\")\nfor i, row in enumerate(cm, 1):\n    print(f\"Q{i}   {row}\")\n\nprint(\"\\nInterpretation:\")\nprint(f\"Diagonal elements show correct predictions for each quadrant\")\nprint(f\"Off-diagonal elements show misclassifications\")\n\n# Calculate per-class accuracy\nprint(\"\\nPer-Quadrant Accuracy:\")\nfor i in range(len(cm)):\n    class_acc = cm[i, i] / cm[i, :].sum() if cm[i, :].sum() > 0 else 0\n    print(f\"  Quad {i+1}: {class_acc:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Cross-validation was already performed during GridSearchCV\n",
    "# Each of the 144 parameter combinations was evaluated using 5-fold CV\n",
    "# The best model (shown above) achieved the best average CV score\n",
    "\n",
    "print(\"GridSearchCV already performed cross-validation!\")\n",
    "print(f\"\\nBest model's cross-validation performance:\")\n",
    "print(f\"  Mean CV Accuracy: {grid_search.best_score_:.4f}\")\n",
    "print(f\"  Across {grid_search.cv} folds\")\n",
    "\n",
    "# Let's also look at the CV scores for the best model specifically\n",
    "best_idx = grid_search.best_index_\n",
    "cv_results = grid_search.cv_results_\n",
    "\n",
    "print(f\"\\nBest model's scores across each fold:\")\n",
    "for fold in range(grid_search.cv):\n",
    "    fold_score = cv_results[f'split{fold}_test_score'][best_idx]\n",
    "    print(f\"  Fold {fold + 1}: {fold_score:.4f}\")\n",
    "\n",
    "print(f\"\\nStandard deviation: {cv_results['std_test_score'][best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Feature Importance Analysis\n",
    "\n",
    "Let's see which features (including one-hot encoded conference categories) are most important for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract feature importance from the trained Random Forest\nbest_model = grid_search.best_estimator_\nfeature_importance = best_model.named_steps['classifier'].feature_importances_\n\n# Get feature names after preprocessing\nnum_feature_names = numeric_features\ncat_feature_names = best_model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\nall_feature_names = num_feature_names + list(cat_feature_names)\n\n# Create a dataframe for visualization\nimportance_df = pd.DataFrame({\n    'Feature': all_feature_names,\n    'Importance': feature_importance\n}).sort_values('Importance', ascending=False)\n\nprint(\"Top 10 Most Important Features:\")\nprint(importance_df.head(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Make Predictions on New Data\n",
    "\n",
    "The beauty of pipelines: preprocessing is applied automatically to new data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Make predictions on a hypothetical new team\n# Create a team with specific stats\n\nnew_team = pd.DataFrame([{\n    'Wins': 25,\n    'Losses': 6,\n    'Conference': 'Big Ten',\n    'Non_D1_Wins': 2\n}])\n\nprint(\"New team statistics:\")\nprint(new_team)\nprint(f\"\\nRecord: {new_team['Wins'].values[0]}-{new_team['Losses'].values[0]}\")\n\n# Make prediction - GridSearchCV delegates to best_estimator_ automatically\nprediction = grid_search.predict(new_team)\nprobability = grid_search.predict_proba(new_team)\n\nprint(f\"\\nPredicted Quadrant: {prediction[0]}\")\nprint(f\"\\nProbabilities for each quadrant:\")\nfor i, prob in enumerate(probability[0], 1):\n    print(f\"  Quad {i}: {prob:.3f} ({prob*100:.1f}%)\")\n\n# Show the most likely quadrant\nmost_likely = prediction[0]\nconfidence = probability[0][most_likely - 1]\nprint(f\"\\n✓ This team is most likely in Quadrant {most_likely} (confidence: {confidence:.1%})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Data Acquisition**: Used `pandas.read_html()` to scrape NCAA NET rankings from the web\n",
    "2. **Feature Engineering**: \n",
    "   - Parsed record strings (e.g., \"25-6\") into separate Wins and Losses columns\n",
    "   - Extracted Non-Division I wins\n",
    "   - Selected only relevant features for modeling\n",
    "3. **Balanced Multi-Class Classification**: Built a 4-class classifier with equal representation\n",
    "4. **Minimal Feature Set**: Used only 4 carefully selected features:\n",
    "   - Wins, Losses (numeric)\n",
    "   - Conference (categorical)\n",
    "   - Non-D1 Wins (numeric)\n",
    "5. **Pipeline Construction**: Built a complete ML pipeline with:\n",
    "   - Separate preprocessing for numeric and categorical features\n",
    "   - Feature scaling for numeric data\n",
    "   - One-hot encoding for conference names\n",
    "   - Model training\n",
    "6. **Hyperparameter Tuning with GridSearchCV**:\n",
    "   - Tested 144 different parameter combinations (3 × 4 × 3 × 3)\n",
    "   - Used 5-fold cross-validation for each combination\n",
    "   - Automatically selected best hyperparameters\n",
    "   - Prevented overfitting through systematic validation\n",
    "7. **Benefits of This Approach**:\n",
    "   - **Pipelines** prevent data leakage and ensure consistent preprocessing\n",
    "   - **GridSearchCV** finds optimal hyperparameters automatically\n",
    "   - **Cross-validation** provides robust performance estimates\n",
    "   - Combined approach is production-ready\n",
    "8. **Model Evaluation**: Used multiple metrics including:\n",
    "   - Overall accuracy\n",
    "   - Per-class precision, recall, F1-score\n",
    "   - Confusion matrix\n",
    "   - Cross-validation scores across parameter combinations\n",
    "\n",
    "### Real-World Application:\n",
    "This pipeline demonstrates:\n",
    "- Professional ML workflow with hyperparameter optimization\n",
    "- How to systematically search large parameter spaces\n",
    "- Balancing model complexity vs. performance\n",
    "- Making data-driven decisions about model configuration\n",
    "\n",
    "### Try It Yourself:\n",
    "- Expand the parameter grid: add `max_features`, `min_samples_leaf`\n",
    "- Try RandomizedSearchCV for larger parameter spaces\n",
    "- Test different classifiers: Logistic Regression, Gradient Boosting, SVM\n",
    "- Create interaction features: `Win_Percentage = Wins / (Wins + Losses)`\n",
    "- Compare performance: GridSearchCV vs. default parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}